Denoising

Dataset source https://www.kaggle.com/datasets?search=object+detection

comparison: BilateralFilter, GaussianBlur

Description: Для вирішення спробував використати U модель.

В тестових варіантах  випробувано:

greyscale варінт для зменшення кількості параметрів в мережі, але використання більших зображень 300*300
для останнього шaру конволюції - використання Relu активації - після кількох епох результат був поганий
також софт-мкс із 255 класами - але ноутбук не потягнув оброблення такого масиву інформації
Вирішив використання 3 канальних зображень. І зменшити розмір зображень до 128 * 128.

Тренувальні дані 10 000+ картинок предметів, пейзажів, тварин і людей. До картинок застосовано шум на основі розподілу гауса. Та в 2% випадках ефект скречінгу - видалення випадкових смуг пікселів, або закрашення їх в чорний.

Мережа пройшла треновання орієнтовно 45-50 епох

Візуальний ефект на мою думку не задовільний. Хоча в певних випадках кращий за Білатеральний Гаусовий фільтри. Visually: 4/10

Варіант рішення змінити архітеркуру шарів. І добавити ще 1 цикл енкодингу декодингу. Оскільки зараз найменший філтр розміром 88 пікселів. Можливо можна спробувати із 44 пікселі.

Алгоритм поступається Білатеральному та Гаусовому в швидкодії.

data processing time :

bilateralFilter - 0.1 sec / 100 pic

GaussianBlur - 0.03 sec / 100 pic

u_filter - 4.4 sec / 100 pic

Висновки. Алгоритм ще має потенціал навчання і покращити ефективність на пару відсотків. Плани: завершити тренунування алгоритму додатковими 10-15 епохами. Якщо ефект буде не задовільний спробувати варіант з глибшим процесом енкодингу декодингу для фільтрів меншого розміру. Спробувати ще раз  greyscale алгоритм, але з більшим резолюшеном картинок. Спробувати greyscale - квантування до 4 кольорів - кластеризацію - dithering Допрацювати інтерфейс і автоматизацію можливості загружати картинки довільного резолюшену.